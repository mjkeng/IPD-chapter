## Frequentist estimation

In this section, a cost-utility analysis is undertaken, comparing the costs and outcomes (i.e. QALYs) associated with 10TT versus usual care. The primary outcomes are incremental costs and effects and the incremental net monetary benefit (NMB) of 101TT versus usual care at 2 years, as per trial follow-up. No extrapolation is undertaken, and cost-effectiveness thresholds of £20,000 and £30,000 are being considered.

### Cost and utility equations

As per [@patel_costeffectiveness_2018], we will compare differences in mean costs and utilities using linear regression models relating participants' outcomes of interest (i.e. costs or change in QALYs at 2 years) with important variables.

---

Extra notes

A _linear regression_ is a type of predictive analysis model that estimates the relationship between an independent variable (e.g. cost at 2 years) and one or more explanatory (dependent) variables (e.g. participant's trial arm or baseline age). In mathematical terms, if $y$ denotes the independent variable, $x_1, \ldots, x_n$ are the independent variables and there are $k$ observation vectors $\{y_i, x_{i1}, \ldots, x_{in} \}_{i=1}^k$, the relationship between $y$ and $x_i$ is assumed to be of the form

$$y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_n x_{in} + \epsilon_i, \quad i = 1, \ldots, k.$$

Here, $\epsilon$ is the _noise_ variable, an unobserved random variable, and $\beta_i$ are the coefficients quantifying the effect of the $i$th predictor $x_i$ onto the outcome variable $y$ while holding all other predictors constant. Specifically, the value of the coefficient $\beta_i$ represents the change in the outcome $y$ if the value of $x_i$ is increased by one unit, while all other parameters remain unchanged.

Further details describing the use of the `lm` command, including possible arguments, output values as well as references to the topic of linear regression in general, can be found on the [`lm` help webpage](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

---

For the simplest scenario, let us start with unadjusted models, i.e. regress participants' outcomes of interest on their treatment arm only:

```{r unadjusted}

# modelling difference in costs
m_cost <- lm(cost ~ arm, data = df_analysis)
summary(m_cost)$coefficients

# modelling difference in utilities
m_qol <- lm(qaly ~ arm, data = df_analysis)
summary(m_qol)$coefficients

```

The models suggest an incremental difference of £`r round(as.numeric(m_cost$coefficients["arm1"]))` and `r round(as.numeric(m_qol$coefficients["arm1"]), digits = 2)` QALYs between the arms, which can be extracting using the code `round(as.numeric(m_cost$coefficients["arm1"]))` and `round(as.numeric(m_qol$coefficients["arm1"]), digits = 2)` respectively.

_Note._ Results from such unadjusted models could also be extracted by calculating mean costs and QALYs in each arm (and finding the difference):

```{r unadjustedManual}

# calculate mean costs & QALYs in each arm
df_analysis %>%
  group_by(arm) %>%
  summarise(mean(qaly), mean(cost))

```

Often, however, it is deemed important to adjust models for further characteristics, for example because of a baseline imbalance between the treatment arms. We can check for the imbalance using the two-sampled `t.test` command:

```{r imbalances}

### Check for imbalances ###

# imbalance in sex between treatment arms
t.test(as.numeric(sex) ~ arm, data = df_analysis)

# imbalance in baseline age between treatment arms
t.test(age ~ arm, data = df_analysis)

# imbalance in baseline utilities between treatment arms
t.test(qol_0 ~ arm, data = df_analysis)

```

As it happens, there is no evidence for baseline imbalances in the dataset. However, even when this is the case, it may still be important to adjust for certain characteristics.

For illustration purposes, we will additionally adjust the cost model for baseline age and gender, and utilities model for baseline age, gender and utility at baseline. This can be done using the same `lm` syntax, but with relevant independent predictors added to the right-hand side of the formula:

```{r adjusted}

### Adjusted analysis ###

# modelling difference in costs (adjusted for age & gender)
m_cost <- lm(cost ~ arm + sex + age, data = df_analysis)
summary(m_cost)$coefficients

# modelling difference in utilities (adjusted for age & gender)
m_qol <- lm(qaly ~ arm + sex + age + qol_0, data = df_analysis)
summary(m_qol)$coefficients

```

_Note._ Whilst this is not an approach taken in this chapter, allowing for correlation between individual costs and utilities may be important, in which case seemingly unrelated regressions (SUR) model could be used. In R, SUR equations can be modelled using functions from the [systemfit](https://cran.r-project.org/web/packages/systemfit/systemfit.pdf) package. An example code is provided below.

```{r adjustedSUR}

# define equations to be fitted
eq_cost <- cost ~ arm + sex + age
eq_qol <- qaly ~ arm + sex + age + qol_0

# save equations into a list (optional step)
system <- list(cost = eq_cost, qol = eq_qol)

# fit SUR model
# note that in the current version, default estimation method is OLS
# therefore "method" argument needs to be set to "SUR"
m_sur <- systemfit::systemfit(system, data = df_analysis, method = "SUR")

# check the output
# coefficients differ very slighly from m_cost and m_qol
summary(m_sur)

```

### Net monetary benefit

We will now calculate the monetary net benefit using the £20,000 and £30,000 willingness-to-pay thresholds. 

First we need to summarise cost and utility differences by arm using the fitted models:

```{r dfInc}

# name resulting variable df_inc
df_inc <- df_analysis %>%
  # predict cost & qaly from regressions
  mutate(cost_predict = predict(m_cost),
         qaly_predict = predict(m_qol)) %>%
  # produce summary statistics by the treatment arm variable
  group_by(arm) %>%
  # summarise mean cost and utility differences
  summarise(cost = mean(cost_predict), qaly = mean(qaly_predict),
            .groups = 'drop') %>%
  # reshape into a wide format
  pivot_longer(cols = c("cost", "qaly")) %>%
  pivot_wider(names_from = c(name, arm), values_from = value) %>%
  # calculate incremental cost and utilities
  mutate(cost_inc = cost_1 - cost_0,
         qaly_inc = qaly_1 - qaly_0
  )

# view results
head(df_inc, 3)

```

And now calculate the net monetary benefit

```{r nb}

# vector with willingness-to-pay thresholds
r <- c(20000, 30000)

# transform r into a dataframe for subsequent merging
df_r <- tibble(r = r)

# name resulting variable df_nb
df_nb <- merge(df_inc, df_r) %>%
  # calculate NMB
  mutate(nb = qaly_inc * r - cost_inc)

# view results
df_nb

```

Thus, the net monetary benefit of the 10TT intervention vs usual care is `r df_nb %>% filter(r == 20000) %>% select(nb) %>% pull() %>% round()` with the £20,000 willingness-to-pay threshold (obtained by code `df_nb %>% filter(r == 20000) %>% select(nb) %>% pull() %>% round()`), and `r df_nb %>% filter(r == 30000) %>% select(nb) %>% pull() %>% round()`  with the £30,000 willingness-to-pay threshold.

### Bootstrap and uncertainty

In this section we will perform non-parametric bootstrap by sampling participants with replacement. For each sampling loop, `r nrow(df_analysis)`  participants will be sampled, where `r nrow(df_analysis)` is the number of participants in our original (complete case) dataset and can be obtained by running the `nrow(df_analysis)` command. The analysis from the previous section will be repeated on this sampled dataset. We will perform 200 such sampling loops and summarise results to obtain confidence intervals around the estimates using the percentile method [@gray2011applied]. 

_Note._ The number of 200 sampling loops was chosen as a compromise between the speed of the execution of the illustrative code and robustness of the results. Typically, 1,000 or more bootstrap samples are required.

For reproducibility purposes, we start with setting the seed of R's random number generator using the command `set.seed`. This way we can ensure when the code is run again, participants are sampled in the same way and therefore the same results are obtained.

```{r set_seed}

set.seed(123)

```

For each loop, sampling will be done using the `sample_n` command from the `tidyverse` package. Before we do the sampling however, let us re-write repetitive chunks of the code in the previous section as functions: 

```{r functions}

get_inc <- function(df) {
  
  # function to summarise cost and utility differences by arm
  # Argument: df (dataframe, e.g. df_analysis)
  # Here, the dataframe must contain "arm" with values 1 & 0, "cost_disc" and "qaly_disc" columns
  # However all that can be parameterised to create a more flexible function
  df_inc <- df %>%
    # produce summary statistics by the treatment arm variable
    group_by(arm) %>%
    # summarise mean cost and utility differences
    summarise(cost = mean(cost), qaly = mean(qaly),
              .groups = 'drop') %>%
    # reshape into a wide format
    pivot_longer(cols = c("cost", "qaly")) %>%
    pivot_wider(names_from = c(name, arm), values_from = value) %>%
    # calculate incremental cost and utilities
    mutate(cost_inc = cost_1 - cost_0,
           qaly_inc = qaly_1 - qaly_0
    )
  
  return(df_inc)
}

# function to summarise cost and utility differences by arm
# Arguments: df_inc (dataframe; as generated by get_inc); r (vector of thresholds)

get_nb <- function(df_inc, r) {
  
  df_r <- tibble(r = r)
  df_nb <- merge(df_inc, df_r) %>%
    mutate(nb = qaly_inc * r - cost_inc)
  
  return(df_nb)
}

```

We are now ready to generate the bootstrap dataset.

```{r bootstrap}

# number of simulations
n_sim <- 200

# initialise output dataset
df_boot <- NULL

# loop across simulations s in 1:n_sim
for (s in 1:n_sim) {
  # sample patients from df_analysis
  df_analysis_s <- sample_n(df_analysis, size = nrow(df_analysis), replace = TRUE)
  # calculate incremental cost & utility differences by simulation
  df_inc_s <- get_inc(df = df_analysis_s) %>%
    # add simulation number
    mutate(sim = s) %>%
    # re-arrange columns
    select(sim, everything())
  # add to the output
  df_boot <- rbind(df_boot, df_inc_s)
}

head(df_boot, 3)

```

_Note_ The above calculations are performed in a sequential loop but are independent of each other. It is therefore possible to parallelise the code, e.g. using the [`foreach` package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html.

Having bootstrapped the dataset, we are ready to calculate the net monetary benefit. We will want to construct the cost-effectiveness acceptability curve, and therefore we need to calculate the NMB for a range of thresholds.

```{r nbBoot}

# vector with thresholds
r <- seq(from = 0, to = 100000, by = 100)

# calculate NMB
df_boot <- get_nb(df_inc = df_boot,  r = r)

# view result
head(df_boot, 3)

```

For the percentile method of calculating 95% confidence intervals, we need to order the vector with the outcome of interest (e.g. the 1,000 cost differences in treatment arm) and take the 25th and 975th values. Since this is likely going to be needed more than once, it may be efficient to turn this process into a function:

```{r bootstrapCI}

# Function: get confidence interval from a vector using a percentile method
# Arguments: vector with outcomes (vec; e.g. cost_delta_0); confidence level (level; optional argument with default value of 0.95)
get_bootstrap_ci <- function(vec, level = 0.95) {
  # order the vector
  vec <- sort(vec)
  n <- length(vec)
  # calculate percentile to be cut off
  temp <- (1 - level) / 2
  # read off values at respective coordinates
  lower <- vec[max(floor(n * temp), 1)]
  upper <- vec[min(ceiling(n * (1 - temp)), n)]
  return(list(l = lower, u = upper))
}

```

We are now ready to generate confidence intervals. For example, let us add uncertainty estimates to the point estimates in the `df_inc` dataset:

```{r}

# create a copy of the original dataset (optional step)
df_inc_with_ci <- df_inc

# loop through all columns of df_inc
for (colname in colnames(df_inc)) {
  
  # uncertainty estimate for a given columns
  v_ci <- get_bootstrap_ci(df_boot[, colname])
  
  # numbers of digits to round the output to
  # 0 for cost columns; 1 for QALY columns
  # could re-write this as a function too
  digits <- if (grepl("cost", colname)) 0 else
    if (grepl("qaly", colname)) 1
  
  # add uncertainty estimate to the point estimate
  df_inc_with_ci[, colname] <- str_c(
    round(df_inc_with_ci[, colname], digits = digits), 
    " (", 
    round(v_ci$l, digits = digits), 
    "; ", 
    round(v_ci$u, digits = digits),
    ")"
  )
}

# view the output and potentially save as a .csv
df_inc_with_ci

```

As the last step, let us plot the bootstrap estimates on the the cost-effectiveness plane.

```{r cePlane}

xmax <- max(abs(df_boot$qaly_inc))
ymax <- max(abs(df_boot$cost_inc))
p <- ggplot(df_boot, aes(x = qaly_inc, y = cost_inc)) + 
  geom_point() + 
  xlab("Incremental QALYs") + ylab("Incremental Cost") + 
  coord_cartesian(xlim = c(-xmax, xmax), ylim = c(-ymax, ymax)) + 
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + 
  theme_bw()
p

```

_Note._ For the sake of completeness, the bootstrapping in this section was done using only `tidyverse` commands. However, the reader may wish to explore purpose-built bootstrapping packages, such as [boot](https://cran.r-project.org/web/packages/boot/index.html), which could be useful for simple procedures. 

### Cost-effectiveness acceptability curve

As it happens, almost all points in the cost-effectiveness place lie in the north-east quadrant, i.e. the 10TT intervention is both more costly and less effective. Let us quantify this effect by constructing the cost-effectiveness acceptability curve.

We will use the `df_boot` dataset for this purpose. First, we need to assess whether each entry in this dataset (i.e. each simulation / threshold combination) is cost-effective:

```{r ceacCE}

# for the simulation to be cost-effective, need 
# (1) EITHER QALY effect to be positive and NMB to be non-negative
# (2) OR QALY effect to be negative and NMB to be non-positive
df_boot <- df_boot %>%
  mutate(ce = (qaly_inc > 0 & nb <= 0) | (qaly_inc < 0 & nb >= 0))

head(df_boot, 3)

```

This can now be translated into a probability of being cost-effective for each willingness-to-pay threshold value

```{r ceacP}

# calculate probability of being cost-effective for each threshold r
# equivalent to the proportion of CE column entries that are equal to 1
# Since CE column entries are either 1 or 0, this is equivalent to the mean
df_ceac <- df_boot %>%
  group_by(r) %>%
  summarise(p_ce = mean(ce))

head(df_ceac, 3)

```

And results can be visualised using standard `ggplot` commands

```{r ceacPlot}

p <- ggplot(df_ceac, aes(x = r, y = p_ce)) +
  # connect observations
  geom_line() +
  # change y-axis limits
  coord_cartesian(ylim = c(0, 1)) +
  # change axis titles
  xlab("Willingness-to-pay threshold") +
  ylab("Probability of the treatment being cost-effective") +
  # display probability labels as percentages
  scale_y_continuous(labels = scales::percent)
p

```

As expected, the probability of the 10TT intervention to be cost-effective is very low at any willingness-to-pay threshold up to £100,000.